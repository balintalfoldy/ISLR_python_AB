{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression – Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.\n",
    "\n",
    "The null hypotheses state, that there is no relationship between the predictors (TV, radio, newspaper budget) and sales, meaning, that H0: β1 = β2 = β3 = 0.\n",
    "The p-values of the individual predictors suggest, that there is a significant relationship between TV budget and sales, and between radio budget and sales, therefore we can reject the null hypothesis. The p-value of newspaper, however is high enough to not to reject the null hypothesis. This suggest, that newspaper budget has no significant effect on sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Carefully explain the differences between the KNN classifier and KNN regression methods.\n",
    "\n",
    "Classifier (local probability): Can be used to colve classification problems, with qualitative response. The KNN classiﬁer ﬁrst identiﬁes the K points in the training data that are closest to x0, represented by N0. It then estimates the conditional probability for class j as the fraction of points in N0 whose response values equal j.\n",
    "Finally, KNN applies Bayes rule and classiﬁes the test observation x0 to the class with the largest probability. \n",
    "\n",
    "Regression (local average): Can be used to solve regression problems, with quantitative response. Given a value for K and a prediction point x0, KNN regression ﬁrst identiﬁes the K training observations that are closest to x0, represented by N0. It then estimates f(x0) using the average of all the training responses in N0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Suppose we have a data set with ﬁve predictors, X1 = GPA,X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to ﬁt the model, and get ˆ β0 = 50 , ˆ β1 = 20, ˆ β2 =0 .07, ˆ β3 = 35 , ˆ β4 =0 .01, ˆ β5 =−10.\n",
    "\n",
    ">(a) For a fixed value of IQ and GPA, salaries of males is higher than females if the GPA > 3.5. So iii., is the right answer.\n",
    "\n",
    ">(b) 137100$\n",
    "\n",
    ">(c) False. We need to check the associated standard error, p-value and t-statistic, in order to draw further conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then ﬁt a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 + β1X + β2X2 + β3X3 + ε.\n",
    "\n",
    ">(a) Suppose that the true relationship between X and Y is linear, i.e. Y=β0+β1X+ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "We would expect the training RSS of cubic regression to be lower, even if the true relationship is linear. This is because the cubic regression is more flexible, with lower bias, and follows the training data more closely.\n",
    "\n",
    ">(b) Answer (a) using test rather than training RSS.\n",
    "\n",
    "We would expect, that the linear regression has lower test RSS, because the true relationship is linear. The linear regression is less flexible and has lower variance, then the cubic regression.\n",
    "\n",
    ">(c) Suppose that the true relationship between X and Y is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "\n",
    "Same as (a).\n",
    "\n",
    ">(d) Answer (c) using test rather than training RSS.\n",
    "\n",
    "There is not enough information to tell. If the relationship is more linear, the linear model might have lower test RSS. However, if it is more non-linear, than the linear model migh fail to capture the true relationship resulting in high bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
